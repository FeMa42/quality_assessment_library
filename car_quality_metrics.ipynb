{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Car Quality Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate the Quality using the Loaded Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using models from package installation directory: /mnt/damian/Projects/quality_assessment_library/car_quality_estimator/models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/damian/Projects/quality_assessment_library/car_quality_estimator/individual_image_classifier.py:393: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  classifier.load_state_dict(torch.load(weights_path, map_location=device))\n",
      "/mnt/damian/miniconda3/envs/trellis_qa/lib/python3.10/site-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator PCA from version 1.5.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from car_quality_estimator.car_quality_metric import load_car_quality_score\n",
    "\n",
    "test_image_dir = \"example_data/2c21b97ff3dc4fc3b1ef9e4bb0164318\"\n",
    "car_quality_metric = load_car_quality_score(use_combined_embedding_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all images from the test directory and store the PIL images in a list\n",
    "all_images = glob.glob(os.path.join(test_image_dir, \"*.png\"))\n",
    "all_images = [PIL.Image.open(image).convert(\"RGB\") for image in all_images]\n",
    "reference_batch = all_images\n",
    "\n",
    "# let's generate some distorted views using Gaussian noise\n",
    "def create_noised_image(image_list):\n",
    "    all_images_np = np.array(image_list) / 255.0\n",
    "    all_images_distorted_np  = all_images_np + np.random.randn(*all_images_np.shape) * 0.0055\n",
    "    all_images_distorted_np = np.clip(all_images_distorted_np, 0, 1)\n",
    "    all_images_distorted_pil = [PIL.Image.fromarray((img * 255).astype(np.uint8)) for img in all_images_distorted_np]\n",
    "    return all_images_distorted_pil\n",
    "generated_views = create_noised_image(reference_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'avg_quality_score': 0.18115516,\n",
       " 'avg_entropy': 0.062057108,\n",
       " 'avg_combined_score': 0.16658795,\n",
       " 'quality_std': 0.36009538,\n",
       " 'num_samples': 21}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quality score with reference images\n",
    "car_quality_metric.compute_scores_no_reference(generated_views)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'generated_metrics': {'avg_quality_score': 0.23797481,\n",
       "  'avg_entropy': 0.11658727,\n",
       "  'avg_combined_score': 0.2111067,\n",
       "  'quality_std': 0.38184297,\n",
       "  'num_samples': 21},\n",
       " 'reference_metrics': {'avg_quality_score': 0.22754328,\n",
       "  'avg_entropy': 0.0401953,\n",
       "  'avg_combined_score': 0.21761395,\n",
       "  'quality_std': 0.40535438,\n",
       "  'num_samples': 21},\n",
       " 'quality_gap': -0.010431528,\n",
       " 'score_distribution_metrics': {'kl_divergence_kde': -0.04320147668210633,\n",
       "  'jensen_shannon_distance': 0.24816035841395828,\n",
       "  'wasserstein_distance': 0.033203934826216204},\n",
       " 'kid_metrics': {'kid_score': 6415.784951636902,\n",
       "  'n_gen_samples': 21,\n",
       "  'n_ref_samples': 21}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quality score with reference images\n",
    "car_quality_metric.compare_with_reference(\n",
    "    generated_views, reference_batch, compute_kid=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trellis_qa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

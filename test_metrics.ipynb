{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from typing import List, Optional, Union\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics:\n",
    "    \"\"\"\n",
    "    A class to compute various image quality metrics between input and target images.\n",
    "    Metrics are computed framewise and logged locally via the provided csv logger.\n",
    "    The metrics are aggregated per image and the total mean (over all made computations) can be obtained using `get_total_metrics`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        device: Optional[str] = \"cuda\",\n",
    "        clip_model_path: str = \"openai/clip-vit-large-patch14\",\n",
    "        clip_cache_dir: str = \"./models/clip-vit-large-patch14\",\n",
    "    ):\n",
    "        from torchmetrics.functional.image import (\n",
    "            peak_signal_noise_ratio,  # PSNR, higher\n",
    "            learned_perceptual_image_patch_similarity,  # LPIPS, lower\n",
    "            structural_similarity_index_measure,  # SSIM, higher\n",
    "            spectral_distortion_index,  # D_lambda, lower\n",
    "            error_relative_global_dimensionless_synthesis,  # ERGAS, lower\n",
    "            relative_average_spectral_error,  # RASE, lower\n",
    "            root_mean_squared_error_using_sliding_window,  # RMSE wind, lower\n",
    "            spectral_angle_mapper,  # SAM, absolute value of the spectral angle, lower\n",
    "            multiscale_structural_similarity_index_measure,  # MS SSIM, higher\n",
    "            universal_image_quality_index,  # higher\n",
    "            visual_information_fidelity,  # higher\n",
    "            spatial_correlation_coefficient,  # higher\n",
    "        )\n",
    "        # FrÃ©chet inception distance (FID)\n",
    "        from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "        # Inception Score (IS) which is used to access how realistic generated images are\n",
    "        from torchmetrics.image.inception import InceptionScore\n",
    "        # Kernel Inception Distance (KID) which is used to access the quality of generated images\n",
    "        from torchmetrics.image.kid import KernelInceptionDistance\n",
    "        from torchmetrics.functional.regression import mean_squared_error\n",
    "        from transformers import CLIPImageProcessor, CLIPModel, CLIPTokenizer\n",
    "\n",
    "        clip_model = CLIPModel.from_pretrained(\n",
    "            clip_model_path, cache_dir=clip_cache_dir\n",
    "        ).to(device)\n",
    "        clip_preprocess = CLIPImageProcessor.from_pretrained(\n",
    "            clip_model_path, cache_dir=clip_cache_dir\n",
    "        )\n",
    "\n",
    "        def _CLIP_score(input, target, device=device):\n",
    "            # if input.shape[0] > 1:\n",
    "            #     return np.nan\n",
    "\n",
    "            # Calculate the embeddings for the images using the CLIP model\n",
    "            with torch.no_grad():\n",
    "                _input = clip_preprocess(\n",
    "                    input * 0.5 + 0.5, do_rescale=False, return_tensors=\"pt\"\n",
    "                )[\"pixel_values\"]\n",
    "                _target = clip_preprocess(\n",
    "                    target * 0.5 + 0.5, do_rescale=False, return_tensors=\"pt\"\n",
    "                )[\"pixel_values\"]\n",
    "\n",
    "                emb_input = clip_model.get_image_features(_input.to(device))\n",
    "                emb_target = clip_model.get_image_features(_target.to(device))\n",
    "\n",
    "                # Calculate the cosine similarity between the embeddings\n",
    "                cos_sim = torch.nn.functional.cosine_similarity(\n",
    "                    emb_input, emb_target)\n",
    "                if len(cos_sim) > 1:\n",
    "                    cos_sim = cos_sim.mean()\n",
    "\n",
    "                # emb_input, emb_target, _input, _target = emb_input.cpu(), emb_target.cpu(), _input.cpu(), _target.cpu()\n",
    "                del emb_input, emb_target, _input, _target\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "                return cos_sim.item()\n",
    "\n",
    "        def spectral_mse(input, target):\n",
    "            orig_dtype = input.dtype\n",
    "\n",
    "            fft1 = torch.fft.fft2(input.to(torch.float32))\n",
    "            fft2 = torch.fft.fft2(target.to(torch.float32))\n",
    "            return ((fft1.abs() - fft2.abs()) ** 2).mean().to(orig_dtype)\n",
    "\n",
    "        self.fid_metric = FrechetInceptionDistance(\n",
    "            feature=64, normalize=True).to(device)\n",
    "\n",
    "        def _FID_score(input, target, device=\"cuda\"):\n",
    "            orig_dtype = input.dtype\n",
    "            self.fid_metric.update(target.to(device), real=True)\n",
    "            self.fid_metric.update(input.to(device), real=False)\n",
    "            if self.fid_metric.real_features_num_samples > 1 and self.fid_metric.fake_features_num_samples > 1:\n",
    "                fid_score = self.fid_metric.compute()\n",
    "            else:\n",
    "                fid_score = torch.tensor(float(0.0))\n",
    "            return fid_score.to(orig_dtype)\n",
    "\n",
    "        self.is_metric = InceptionScore(normalize=True).to(device)\n",
    "\n",
    "        def _IS_score(input, target, device=\"cuda\"):\n",
    "            orig_dtype = input.dtype\n",
    "            is_score = self.is_metric.update(input.to(device))\n",
    "            is_score, _ = self.is_metric.compute()\n",
    "            return is_score.to(orig_dtype)\n",
    "\n",
    "        self.kid_metric = KernelInceptionDistance(\n",
    "            subset_size=1, normalize=True).to(device)\n",
    "\n",
    "        def _KID_score(input, target, device=\"cuda\"):\n",
    "            orig_dtype = input.dtype\n",
    "            self.kid_metric.update(target.to(device), real=True)\n",
    "            self.kid_metric.update(input.to(device), real=False)\n",
    "            if len(self.kid_metric.real_features) > 1 and len(self.kid_metric.fake_features) > 1:\n",
    "                kid_score, _ = self.kid_metric.compute()\n",
    "            else:\n",
    "                kid_score = torch.tensor(float(0.0))\n",
    "            return kid_score.to(orig_dtype)\n",
    "\n",
    "        self.metrics = {\n",
    "            \"MSE\": mean_squared_error,\n",
    "            \"CLIP-S\": _CLIP_score,\n",
    "            \"Spectral_MSE\": spectral_mse,\n",
    "            \"D_lambda\": spectral_distortion_index,\n",
    "            \"ERGAS\": error_relative_global_dimensionless_synthesis,\n",
    "            \"PSNR\": peak_signal_noise_ratio,\n",
    "            \"RASE\": relative_average_spectral_error,\n",
    "            \"RMSE_wind\": root_mean_squared_error_using_sliding_window,\n",
    "            \"SAM\": spectral_angle_mapper,\n",
    "            \"MS-SSIM\": multiscale_structural_similarity_index_measure,\n",
    "            \"SSIM\": structural_similarity_index_measure,\n",
    "            \"UQI\": universal_image_quality_index,\n",
    "            \"VIF\": visual_information_fidelity,\n",
    "            \"LPIPS\": learned_perceptual_image_patch_similarity,\n",
    "            \"SCC\": spatial_correlation_coefficient\n",
    "        }\n",
    "        self.distribution_metrics = {\n",
    "            \"FID\": _FID_score,\n",
    "            \"IS\": _IS_score,\n",
    "            \"KID\": _KID_score\n",
    "        }\n",
    "        self.result = torch.zeros(len(self.metrics), device=device)\n",
    "        self.total = 0\n",
    "        self.device = device\n",
    "\n",
    "    def reset_fid(self):\n",
    "        self.fid_metric.reset()\n",
    "\n",
    "    def reset_kid(self):\n",
    "        self.kid_metric.reset()\n",
    "\n",
    "    def reset_is(self):\n",
    "        from torchmetrics.image.inception import InceptionScore\n",
    "        self.is_metric = InceptionScore(normalize=True).to(self.device)\n",
    "\n",
    "    def compute_image(self, input, target):\n",
    "        \"\"\"\n",
    "        compute image metrics framewise and aggregate over image\n",
    "\n",
    "        Args:\n",
    "            input (torch.Tensor): Input tensor of shape (num_frames, channels, height, width)\n",
    "            target (torch.Tensor): Target tensor of shape (num_frames, channels, height, width)\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the average values of the metrics computed over all frames / for a single image\n",
    "        \"\"\"\n",
    "        # compute metrics framewise\n",
    "        assert input.shape == target.shape\n",
    "        num_frames = input.shape[0]\n",
    "        # normalize images\n",
    "        input = input.to(self.device) * 2 - 1\n",
    "        target = target.to(self.device) * 2 - 1\n",
    "\n",
    "        imagewise_output = torch.zeros(len(self.metrics), device=self.device)\n",
    "        for i in range(num_frames):\n",
    "            framewise_output_dict = {}\n",
    "            framewise_output_dict[\"frame\"] = i\n",
    "            # compute all metrics for the frame\n",
    "            for name, metric in self.metrics.items():\n",
    "                framewise_output_dict[name] = metric(\n",
    "                    input[i].unsqueeze(0), target[i].unsqueeze(0)\n",
    "                )\n",
    "            # aggregate metrics over all frames\n",
    "            imagewise_output += torch.tensor([v for k, v in framewise_output_dict.items(\n",
    "            ) if k != \"frame\"], device=self.device)\n",
    "            del framewise_output_dict\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        self.reset_fid()\n",
    "        self.reset_kid()\n",
    "        self.reset_is()\n",
    "        for name, metric in self.distribution_metrics.items():\n",
    "            imagewise_output[name] = metric(input, target)\n",
    "            \n",
    "        self.result += imagewise_output / num_frames\n",
    "        self.total += 1\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        metrics = dict(zip(self.metrics.keys(), imagewise_output / self.total))\n",
    "        metrics.update(dict(zip(self.distribution_metrics.keys(), imagewise_output / self.total)))\n",
    "        return metrics\n",
    "\n",
    "    def get_total_metrics(self):\n",
    "        \"\"\"\n",
    "        Get the average values of the metrics computed over all images.\n",
    "        Returns:\n",
    "            dict: A dictionary containing the average values of the metrics computed over all images.\n",
    "        \"\"\"\n",
    "        metrics = dict(zip(self.metrics.keys(), self.result / self.total))\n",
    "        metrics.update(dict(zip(self.distribution_metrics.keys(), self.result / self.total)))\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = Metrics(device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([21, 576, 576, 3])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "import glob\n",
    "import PIL.Image\n",
    "\n",
    "test_image_dir = \"data/2c21b97ff3dc4fc3b1ef9e4bb0164318\"\n",
    "all_images = glob.glob(os.path.join(test_image_dir, \"*.png\"))\n",
    "all_images = [PIL.Image.open(image).convert(\"RGB\") for image in all_images]\n",
    "torch_image_tensor = torch.tensor(np.array(all_images))\n",
    "# (num_frames, channels, height, width)\n",
    "# torch_image_tensor = torch_image_tensor.permute(0, 3, 1, 2)\n",
    "torch_image_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([576, 576, 3])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_image_tensor[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchmetrics.functional.regression import mean_squared_error\n",
    "\n",
    "mse = mean_squared_error(torch_image_tensor[0], torch_image_tensor[0])\n",
    "mse.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mean(): could not infer output dtype. Input dtype must be either a floating point or complex dtype. Got: Byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch_image_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_image_tensor\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[27], line 173\u001b[0m, in \u001b[0;36mMetrics.compute_image\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# compute all metrics for the frame\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, metric \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 173\u001b[0m     framewise_output_dict[name] \u001b[38;5;241m=\u001b[39m \u001b[43mmetric\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# aggregate metrics over all frames\u001b[39;00m\n\u001b[1;32m    177\u001b[0m imagewise_output \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m framewise_output_dict\u001b[38;5;241m.\u001b[39mitems(\n\u001b[1;32m    178\u001b[0m ) \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframe\u001b[39m\u001b[38;5;124m\"\u001b[39m], device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/miniconda3/envs/vlm/lib/python3.10/site-packages/torchmetrics/functional/image/ergas.py:122\u001b[0m, in \u001b[0;36merror_relative_global_dimensionless_synthesis\u001b[0;34m(preds, target, ratio, reduction)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calculates `Error relative global dimensionless synthesis`_ (ERGAS) metric.\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \n\u001b[1;32m     93\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    119\u001b[0m \n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    121\u001b[0m preds, target \u001b[38;5;241m=\u001b[39m _ergas_update(preds, target)\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ergas_compute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vlm/lib/python3.10/site-packages/torchmetrics/functional/image/ergas.py:79\u001b[0m, in \u001b[0;36m_ergas_compute\u001b[0;34m(preds, target, ratio, reduction)\u001b[0m\n\u001b[1;32m     77\u001b[0m sum_squared_error \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(diff \u001b[38;5;241m*\u001b[39m diff, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     78\u001b[0m rmse_per_band \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt(sum_squared_error \u001b[38;5;241m/\u001b[39m (h \u001b[38;5;241m*\u001b[39m w))\n\u001b[0;32m---> 79\u001b[0m mean_target \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m ergas_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m/\u001b[39m ratio \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt(torch\u001b[38;5;241m.\u001b[39msum((rmse_per_band \u001b[38;5;241m/\u001b[39m mean_target) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m c)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reduce(ergas_score, reduction)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mean(): could not infer output dtype. Input dtype must be either a floating point or complex dtype. Got: Byte"
     ]
    }
   ],
   "source": [
    "metrics.compute_image(torch_image_tensor, torch_image_tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

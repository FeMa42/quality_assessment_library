{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import glob\n",
    "import PIL.Image\n",
    "from typing import List, Optional, Union\n",
    "import numpy as np\n",
    "import torch\n",
    "from metrics import Metrics, preprocess_image\n",
    "\n",
    "device = \"cpu\"\n",
    "test_image_dir = \"data/2c21b97ff3dc4fc3b1ef9e4bb0164318\"\n",
    "\n",
    "def load_images_from_dir(image_dir: str):\n",
    "    all_images = glob.glob(os.path.join(image_dir, \"*.png\"))\n",
    "    all_images = [preprocess_image(PIL.Image.open(image)) for image in all_images]\n",
    "    torch_image_tensor = torch.tensor(np.array(all_images), dtype=torch.float32)\n",
    "    # (num_frames, channels, height, width)\n",
    "    torch_image_tensor = torch_image_tensor.permute(0, 3, 1, 2)\n",
    "    target = torch_image_tensor.to(device)\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/damian/miniconda3/envs/vlm/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/Users/damian/miniconda3/envs/vlm/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: Metric `Kernel Inception Distance` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    }
   ],
   "source": [
    "metrics = Metrics(device=device)\n",
    "target = load_images_from_dir(test_image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    input = target + torch.randn_like(target) * 0.001\n",
    "    input = torch.clamp(input, 0, 1).to(device)\n",
    "    metrics.compute_image(input, target)\n",
    "metrics.get_total_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Car Quality Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from car_quality_estimator.car_quality_metric import load_car_quality_score\n",
    "\n",
    "test_image_dir = \"data/2c21b97ff3dc4fc3b1ef9e4bb0164318\"\n",
    "# load_car_quality_score(model_dir=\"../models\", device=None, use_combined_embedding_model=False)\n",
    "car_quality_metric = load_car_quality_score(\n",
    "    model_dir=\"./models/\", use_combined_embedding_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_dir = \"data/2c21b97ff3dc4fc3b1ef9e4bb0164318\"\n",
    "all_images = glob.glob(os.path.join(test_image_dir, \"*.png\"))\n",
    "all_images = [PIL.Image.open(image).convert(\"RGB\") for image in all_images]\n",
    "reference_batch = all_images\n",
    "# reference_batch = [all_images[i:i+4] for i in range(0, len(all_images), 4)]\n",
    "all_images_np = np.array(all_images) / 255.0\n",
    "all_images_distorted_np  = all_images_np + np.random.randn(*all_images_np.shape) * 0.0005\n",
    "all_images_distorted_np = np.clip(all_images_distorted_np, 0, 1)\n",
    "all_images_distorted_pil = [PIL.Image.fromarray((img * 255).astype(np.uint8)) for img in all_images_distorted_np]\n",
    "generated_views = all_images_distorted_pil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing quality scores for generated models...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'avg_quality_score': 0.16362703,\n",
       " 'avg_entropy': 0.15762094,\n",
       " 'avg_combined_score': 0.14266102,\n",
       " 'quality_std': 0.29280072,\n",
       " 'num_samples': 21}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car_quality_metric.compute_scores_no_reference(generated_views)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for generated models...\n",
      "Generating embeddings for reference models...\n",
      "Computing quality scores for generated models...\n",
      "Computing quality scores for reference models...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'generated_metrics': {'avg_quality_score': 0.16678736,\n",
       "  'avg_entropy': 0.15905564,\n",
       "  'avg_combined_score': 0.1438047,\n",
       "  'quality_std': 0.29521376,\n",
       "  'num_samples': 21},\n",
       " 'reference_metrics': {'avg_quality_score': 0.19731405,\n",
       "  'avg_entropy': 0.07442095,\n",
       "  'avg_combined_score': 0.1805404,\n",
       "  'quality_std': 0.37064323,\n",
       "  'num_samples': 21},\n",
       " 'quality_gap': 0.030526698,\n",
       " 'score_distribution_metrics': {'kl_divergence_kde': -0.04776806698989982,\n",
       "  'jensen_shannon_distance': 0.3114382384936274,\n",
       "  'wasserstein_distance': 0.06992027795109917},\n",
       " 'kid_metrics': {'kid_score': 11027.498195684522,\n",
       "  'n_gen_samples': 21,\n",
       "  'n_ref_samples': 21}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car_quality_metric.compare_with_reference(\n",
    "    generated_views, reference_batch, compute_kid=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Estimating Metrics for MeshFleet benchmark"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Semantic, Geometric and distributional metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os \n",
                "from metrics.metrics_eval import process_metrics_by_viewpoint\n",
                "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
                "\n",
                "gt_folder = \"example_data/Meshfleet_Eval/Ground_Truth\"\n",
                "gen_folder = \"example_data/Meshfleet_Eval/Results_000\"\n",
                "cfg_path  = \"example_data/Meshfleet_Eval/config_meshfleet.json\"\n",
                "\n",
                "metrics_result = process_metrics_by_viewpoint(\n",
                "    ground_truth_folder=gt_folder,\n",
                "    generated_folder=gen_folder,\n",
                "    device=\"cpu\",\n",
                "    config_path=cfg_path,\n",
                ")\n",
                "\n",
                "import json\n",
                "print(json.dumps(metrics_result, indent=2))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Write results into json file"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from metrics.metrics_eval import process_metrics_by_viewpoint, tensor_to_serializable, json_file_to_combined_table\n",
                "json_output = json.dumps(tensor_to_serializable(metrics_result), indent=4)\n",
                "\n",
                "# Save the results in a JSON file in the root folder.\n",
                "output_file = os.path.join(os.path.dirname(gt_folder), \"meshfleet_metrics_results.json\")\n",
                "with open(output_file, 'w') as f:\n",
                "    f.write(json_output)\n",
                "\n",
                "print(f\"Metrics saved to {output_file}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Prompt alignment metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from metrics.metrics import Metrics, GeometryMetrics, ImageBasedPromptEvaluator\n",
                "from metrics.helpers import preprocess_image, process_folder_with_prompt, process_folder_with_metadata_file\n",
                "import os \n",
                "import pandas as pd\n",
                "from tqdm import tqdm\n",
                "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
                "\n",
                "gen_base_folder = \"example_data/Meshfleet_Eval/Results_000\"\n",
                "metadata_file_path=\"example_data/meshfleet_test.csv\"\n",
                "\n",
                "# Example usage\n",
                "prompt_metric = ImageBasedPromptEvaluator() \n",
                "mean_scores, std_scores = process_folder_with_metadata_file(generated_base_folder=gen_base_folder, metadata_file_path=metadata_file_path,\n",
                "                                                            prompt_metric=prompt_metric, \n",
                "                                                            preprocess_image=preprocess_image)\n",
                "print(\"Mean scores:\")\n",
                "print(mean_scores)\n",
                "print(\"Standard deviation scores:\")\n",
                "print(std_scores)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Vehicle based dimension comparison\n",
                "\n",
                "If you want to compare the dimensions without a detailed geometric analysis you can also use `estimate_dimension_differences` from `FlorenceWheelbaseOD`. This method estimates the dimensions and wheelbase of the (vehicle) objects based on the images normalizes them based on the height of the object and compares them. The method returns a dictionary with the differences in the dimensions. The last value is the non normalized height difference. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os \n",
                "from metrics.viewpoint_florence import FlorenceWheelbaseOD\n",
                "from metrics.helpers import evaluate_vehicle_dimensions\n",
                "\n",
                "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
                "\n",
                "# Example usage\n",
                "generated_base_folder = \"example_data/Meshfleet_Eval/Results_000\"\n",
                "metadata_file_path=\"example_data/meshfleet_test.csv\"\n",
                "florence_wheelbase_od = FlorenceWheelbaseOD()\n",
                "average_diff, std_diff = evaluate_vehicle_dimensions(generated_base_folder, metadata_file_path, florence_wheelbase_od)\n",
                "print(f\"Average difference in vehicle dimensions: {average_diff}\")\n",
                "print(f\"Standard deviation of differences: {std_diff}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "trellis_qa",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.16"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "import os \n",
    "import glob\n",
    "from PIL import Image\n",
    "from typing import List, Optional, Union\n",
    "import numpy as np\n",
    "import torch\n",
    "from metrics.metrics import Metrics, GeometryMetrics, ImageBasedPromptEvaluator\n",
    "from metrics.helpers import preprocess_image, preprocess_image_rgba, process_folder_new\n",
    "from metrics.helpers import process_folder_with_prompt_files\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics- Config Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics.metrics_eval import process_metrics_by_viewpoint\n",
    "\n",
    "gt_folder = \"example_data/Meshfleet_Eval/GT_test\"\n",
    "gen_folder = \"example_data/Meshfleet_Eval/Results_Test\"\n",
    "cfg_path  = \"example_data/Meshfleet_Eval/config_small.json\"\n",
    "\n",
    "metrics_result = process_metrics_by_viewpoint(\n",
    "    ground_truth_folder=gt_folder,\n",
    "    generated_folder=gen_folder,\n",
    "    device=\"cuda\",\n",
    "    config_path=cfg_path,\n",
    ")\n",
    "\n",
    "import json\n",
    "print(json.dumps(metrics_result, indent=2))"
    "# original_folder = \"Meshfleet_Eval/Ground_Truth/0ae696bd837219e784b8b7979807184decd5abdb813f0fd7bbfbf6a82bdcc798\"\n",
    "# generated_folder = \"Meshfleet_Eval/Results_000/0ae696bd837219e784b8b7979807184decd5abdb813f0fd7bbfbf6a82bdcc798\"\n",
    "# generated_folder = \"/mnt/damian/Projects/TRELLIS/eval_prompt_alignment/output_flux_lora_maxR/gaussian_0ae696bd837219e784b8b7979807184decd5abdb813f0fd7bbfbf6a82bdcc798\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Semantic/Quality Metrics\n",
    "- For evaluations with very small sample size (n<20), set *compute_distribution_metrics = False* as FID & KID require a minimum of 20 samples to be computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_metrics = process_folder_new(\n",
    "    original_folder=original_folder,\n",
    "    generated_folder=generated_folder,\n",
    "    preprocess_func=preprocess_image,  \n",
    "    metric_class=Metrics,\n",
    "    device=\"cuda\",  \n",
    "    compute_distribution_metrics=False  \n",
    ")\n",
    "print(\"Semantic Metrics:\")\n",
    "for k, v in semantic_metrics.items():\n",
    "    print(f\"{k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Geometric Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For geometry metrics:\n",
    "geometry_metrics = process_folder_new(\n",
    "    original_folder=original_folder,\n",
    "    generated_folder=generated_folder,\n",
    "    preprocess_func=preprocess_image_rgba,  \n",
    "    metric_class=GeometryMetrics,\n",
    "    num_points=100\n",
    ")\n",
    "print(\"Geometry Metrics:\")\n",
    "for k, v in geometry_metrics.items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prompt metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_folder = \"/mnt/damian/Projects/quality_assessment_library/example_data/0ae696bd837219e784b8b7979807184decd5abdb813f0fd7bbfbf6a82bdcc798\"\n",
    "image_scores = process_folder_with_prompt_files(\n",
    "    generated_folder=generated_folder,\n",
    "    preprocess_func=preprocess_image,\n",
    "    prompt_metric=ImageBasedPromptEvaluator() \n",
    ")\n",
    "image_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Metrics for multiple combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "import os \n",
    "import glob\n",
    "from PIL import Image\n",
    "from typing import List, Optional, Union\n",
    "import numpy as np\n",
    "import torch\n",
    "from metrics.metrics import Metrics, GeometryMetrics, ImageBasedPromptEvaluator\n",
    "from metrics.helpers import preprocess_image, preprocess_image_rgba, process_folder_new\n",
    "from metrics.helpers import process_folder_with_prompt_files\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "from metrics.metrics_eval import process_metrics_by_viewpoint, tensor_to_serializable, json_file_to_combined_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write results into json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from metrics.metrics_eval import process_metrics_by_viewpoint, tensor_to_serializable, json_file_to_combined_table\n",
    "metadata_csv = \"/mnt/damian/Projects/TRELLIS/datasets/compact_prompt.csv\"\n",
    "test_prompts = pd.read_csv(metadata_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_folder = \"Meshfleet_Eval/Ground_Truth\"\n",
    "gen_folder = \"Meshfleet_Eval/Results_000\"\n",
    "\n",
    "metadata_csv = \"/mnt/damian/Projects/TRELLIS/datasets/compact_prompt.csv\"\n",
    "\n",
    "metrics_result = process_metrics_by_viewpoint(gt_folder, gen_folder, metadata_csv=metadata_csv, device=\"cuda\")\n",
    "json_output = json.dumps(tensor_to_serializable(metrics_result), indent=4)\n",
    "\n",
    "# Save the results in a JSON file in the root folder.\n",
    "output_file = os.path.join(os.path.dirname(gt_folder), \"metrics_results_config_test.json\")\n",
    "with open(output_file, 'w') as f:\n",
    "    f.write(json_output)\n",
    "\n",
    "print(f\"Metrics saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print metrics from json into a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_filepath = \"Meshfleet_Eval/metrics_results_000.json\"\n",
    "combined_df = json_file_to_combined_table(json_filepath)\n",
    "\n",
    "# print(\"Combined Metrics Table:\")\n",
    "# print(combined_df.to_string())\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trellis_qa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

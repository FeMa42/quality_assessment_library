{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Metrics for evaluating the performance of 3D generative models\n",
                "\n",
                "The repository combines several metrics for evaluating the performance of 3D generative models. We sorted them to the following categories:\n",
                "- Semantic Metrics: Includes metrics like MSE and CLIP-S\n",
                "- Geometry Metrics: \n",
                "    - Rel_BB_Aspect_Ratio_Diff: Bounding box aspect ratio difference\n",
                "    - Rel_Pixel_Area_Diff: Relative pixel area difference\n",
                "    - Squared_Outline_Normals_Angle_Diff: Squared difference of the angle between the outline normals\n",
                "    - Squared_Summed_Outline_Normals_Angle_Diff: Squared difference of the summed outline normals\n",
                "- Distribution Metrics: \n",
                "    - Frechet Inception Distance (FID): Measures the distance between two distributions\n",
                "    - Kernel Inception Distance (KID): Similar to FID but uses a different approach to measure distance\n",
                "    - Inception Score (IS): Measures the quality of generated images\n",
                "- Prompt alignment metrics: \n",
                "    - CLIP-S: Measures the alignment of generated images with text prompts\n",
                "    - ImageReward: Measures the quality of generated images using a reward model\n",
                "- Vehicle Based Dimension comparison\n",
                "    - Width and length comparison (normalized by height)\n",
                "    - Vehicle wheelbase comparison (normalized by height). Uses Florence OD to detect the wheels.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Semantic metrics based on a list of images\n",
                "\n",
                "This block shows the general functionality of the Metrics class for the semantic metrics. This can serve as a template for implementing an evaluation routine for estimating the model performance during training. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import glob\n",
                "import numpy as np\n",
                "from PIL import Image\n",
                "import torch\n",
                "from metrics.helpers import preprocess_image # load_images_from_dir\n",
                "from metrics.metrics import Metrics\n",
                "\n",
                "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
                "\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "\n",
                "# you can find this method also in the metrics.helpers module but we include it here for reference\n",
                "def load_images_from_dir(image_dir: str, device, preprocess_func):\n",
                "    \"\"\"\n",
                "    Load all images from a directory, preprocess them with preprocess_func,\n",
                "    and return a tensor of shape (num_frames, channels, height, width).\n",
                "    \"\"\"\n",
                "    files = sorted(glob.glob(os.path.join(image_dir, \"*.png\")))\n",
                "    images = [preprocess_func(Image.open(f)) for f in files]\n",
                "    arr = np.array([np.array(img) for img in images])\n",
                "    tensor = np.transpose(arr, (0, 3, 1, 2))\n",
                "    tensor = torch.tensor(tensor, dtype=torch.float32).to(device)\n",
                "    return tensor\n",
                "\n",
                "# by defining this class we can compute the metrics several times\n",
                "semantic_metric = Metrics(device=device, compute_distribution_metrics=False)\n",
                "# load the images from the directories\n",
                "gt_folder = \"example_data/Meshfleet_Eval/Ground_Truth/0ae696bd837219e784b8b7979807184decd5abdb813f0fd7bbfbf6a82bdcc798\"\n",
                "gen_folder = \"example_data/Meshfleet_Eval/Results_000/0ae696bd837219e784b8b7979807184decd5abdb813f0fd7bbfbf6a82bdcc798\"\n",
                "input_tensor  = load_images_from_dir(gt_folder, device, preprocess_image)\n",
                "target_tensor = load_images_from_dir(gen_folder,  device, preprocess_image)\n",
                "# compute the metrics\n",
                "semantic_metric.compute_image(input_tensor, target_tensor)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# if you want to specify the metrics you can pass them as metric_list\n",
                "semantic_metric = Metrics(device=device, compute_distribution_metrics=False, metric_list=[\"MSE\", \"CLIP-S\", \"LPIPS\", \"SSIM\", \"PSNR\"])\n",
                "semantic_metric.compute_image(input_tensor, target_tensor)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Process multiple Objects where the generated images are aligned by viewpoints\n",
                "\n",
                "For a detailed evaluation of the model performance the following code can be used. \n",
                "\n",
                "The following code assumes that you have a set of images for each object where the generations and the reference are aligned by viewpoints. The images of each object should be in a directory. Take a look at the example data in `example_data/` to see how the data should be structured. The images should be named in a way that the sorted order of the filenames corresponds to the order of the viewpoints. For example, if you have 8 images for each object, the filenames should be `0.png`, `1.png`, `2.png`, ..., `7.png`. The code will automatically sort the images in each directory and calculate the metrics for each object. \n",
                "\n",
                "To align the images by viewpoints and preprocess the images, you can use the `data_preprocessing.ipynb` notebook. If you use the geometry metrics, this step is necessary.\n",
                "\n",
                "\n",
                "You can configure which metrics should be calculated using the Metrics-Config file (`cfg_path  = \"example_data/config_small.json\"` in the example below). You can select if the metrics should be estimated on a viewpoint basis or over all images. The distribution metrics (FID, KID, IS) are calculated over all images. The semantic metrics (MSE, CLIP-S) can be calculated on a viewpoint basis or over all images. The geometry metrics are calculated on a viewpoint basis. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from metrics.metrics_eval import process_metrics_by_viewpoint\n",
                "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
                "\n",
                "gt_folder = \"example_data/Meshfleet_Eval/Ground_Truth\"\n",
                "gen_folder = \"example_data/Meshfleet_Eval/Results_000\"\n",
                "cfg_path  = \"example_data/config_small.json\"\n",
                "\n",
                "metrics_result = process_metrics_by_viewpoint(\n",
                "    ground_truth_folder=gt_folder,\n",
                "    generated_folder=gen_folder,\n",
                "    device=\"cuda\",\n",
                "    config_path=cfg_path,\n",
                ")\n",
                "\n",
                "import json\n",
                "print(json.dumps(metrics_result, indent=2))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Write results into json file"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from metrics.metrics_eval import process_metrics_by_viewpoint, tensor_to_serializable, json_file_to_combined_table\n",
                "json_output = json.dumps(tensor_to_serializable(metrics_result), indent=4)\n",
                "\n",
                "# Save the results in a JSON file in the root folder.\n",
                "output_file = os.path.join(os.path.dirname(gt_folder), \"metrics_results_config_test.json\")\n",
                "with open(output_file, 'w') as f:\n",
                "    f.write(json_output)\n",
                "\n",
                "print(f\"Metrics saved to {output_file}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Prompt alignment metrics\n",
                "\n",
                "If you want to evaluate a text-to-3D model, you can use the prompt alignment metrics. The method `process_folder_with_prompt_files` assumes that the images and prompts are in the same folder. The prompt files should have the same name as the images, but with a `.txt` extension. For example, if you have an image named `image_1.png`, the corresponding prompt file should be named `image_1.txt`. The prompt files should contain the text prompts used to generate the images."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from metrics.metrics import Metrics, GeometryMetrics, ImageBasedPromptEvaluator\n",
                "from metrics.helpers import preprocess_image, process_folder_with_prompt_files, process_folder_with_prompt\n",
                "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
                "\n",
                "prompt_metric = ImageBasedPromptEvaluator() \n",
                "\n",
                "generated_folder = \"example_data/0ae696bd837219e784b8b7979807184decd5abdb813f0fd7bbfbf6a82bdcc798\"\n",
                "image_scores = process_folder_with_prompt_files(\n",
                "    generated_folder=generated_folder,\n",
                "    preprocess_func=preprocess_image,\n",
                "    prompt_metric=prompt_metric\n",
                ")\n",
                "image_scores"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "If you have a single prompt and a folder with images you can use `process_folder_with_prompt`. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "prompt = \"Compact crossover in azure blue, white roof & mirrors.\"\n",
                "process_folder_with_prompt(\n",
                "    generated_folder=generated_folder,\n",
                "    object_prompts=prompt,\n",
                "    preprocess_func=preprocess_image,\n",
                "    prompt_metric=prompt_metric)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Vehicle based dimension comparison\n",
                "\n",
                "If you want to compare the dimensions without a detailed geometric analysis you can also use `estimate_dimension_differences` from `FlorenceWheelbaseOD`. This method estimates the dimensions and wheelbase of the (vehicle) objects based on the images normalizes them based on the height of the object and compares them. The method returns a dictionary with the differences in the dimensions. The last value is the non normalized height difference. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os \n",
                "from metrics.viewpoint_florence import FlorenceWheelbaseOD\n",
                "\n",
                "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
                "\n",
                "original_folder = \"example_data/Meshfleet_Eval/Ground_Truth/0ae696bd837219e784b8b7979807184decd5abdb813f0fd7bbfbf6a82bdcc798\"\n",
                "generated_folder = \"example_data/Meshfleet_Eval/Results_000/0ae696bd837219e784b8b7979807184decd5abdb813f0fd7bbfbf6a82bdcc798\"\n",
                "\n",
                "# you have to remove the background of the images for the florence based metrics to work properly\n",
                "# remove_background_recursive(generated_folder)\n",
                "florence_wheelbase_od = FlorenceWheelbaseOD()\n",
                "differences = florence_wheelbase_od.estimate_dimension_differences(generated_folder, original_folder, normalize=True)\n",
                "differences"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "trellis_qa",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.16"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
